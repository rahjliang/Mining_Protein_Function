{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "language modeling--making logical, hierarchical repsof words and vocabs. a core feature is sematnic types and relationships. UMLS ahs 135 sematnic types and 54 relations. \n",
    "umls is based off of manual indexing. new concepts are hand curated for sematnic type and relations\n",
    "working from existing resource of umls req humans. \n",
    "***METAMAP API with https://metamap.nlm.nih.gov/\n",
    "#you CAN install metamap command softwar and run from machine. OR you can submit a job to metamap. they'll return your concepts back. The return is fast. upload your documents etc. if they're pubmed IDs, you don't even need documents, just need the PMIDs. \n",
    "#this is important for pubmed. So for pubmed we use this. \n",
    "#if you want to construct a language model and not familiar with the bigger field, then it becomes both labor intensive and hard for others to use. \n",
    "\n",
    "Very cool. Semantic modeling: \n",
    "construcing and maintianing an ontology or other language model is labor intensive\n",
    "what can you use it for? \n",
    "    knowledge management--quantify info i have\n",
    "    information retrieval --document clustering (identify synonymous and similar terms)\n",
    "    understanding your domain --knowledge discovery #buzzword. Elsevier tried and didn't work. knowledge discovery is what I WANT TO DO. \n",
    "    \n",
    "    Automated semantic modeling: modeling a new domain without benefit of informatitions is a labor intensive process. \n",
    "    \n",
    "    \n",
    "    DEEP LEARNING: \n",
    "    1. rapidly changing domain. matches closely to what people understand is AI and machine learning\n",
    "    2. based on a line of research taht relies on functional unit of artificial neurons\n",
    "    3. most of what you encounter that is sucessful dep learning is in image processing. \n",
    "    \n",
    "Deep learning--intro \n",
    "image processing neural networks rely on couple of proprties that don't translate wll to text processing: \n",
    "    image processing relies on learning probabilities about the RGB composition of adjacent pixels\n",
    "    Ambiguity conferred in images is resolvale by adding or changing eatures. \n",
    "    \n",
    "    deep learning for texta pplications: embeddings are a way of learning similar sematnic roles of wrods in text\n",
    "    relies on VERY large text corpora. \n",
    "    #embedings and applications: in addition to automatically learing sematnic roles, embeddings are frequently used in automatic machine translation. \n",
    "    \n",
    "    \n",
    "    can use gensim to learn embeddings from text. gensim by default learns embeddings fro word2vec from Wiki corpus. Caveat: again, corpus size has to be quite large to support proividng enough examples. Overiftting is a real concern for embeddings-based techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#topic modeling`*****\n",
    "Embeddings are commonly learned using Latent Dirichlet Allocation (LDA)\n",
    "LDA is a technique for moedling feature's contribution to class membership\n",
    "LDA is also commonly used for sematnic role labeling. \n",
    "    ****another SRL approach is non-negative matrix factorization. How does this work???? \n",
    "    \n",
    "overwhelmingly LDA is the techinque for semantic role labeling and topic modeling. LDA has limitations though. \n",
    "Python example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'corpora/reuters' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/liangrj/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/reuters.zip/reuters/' not found.  Please use\n  the NLTK Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/liangrj/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-955dbdfec58d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreuters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#construct a corpus from reuters. select text axamples taht are ASSUMED coherent based on labels. Learn a topic model. COmapre and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#assumed is post-constructed coherence. reuters is multitaged. so acq may have many alum also.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/reuters' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/liangrj/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "reuters.categories() #construct a corpus from reuters. select text axamples taht are ASSUMED coherent based on labels. Learn a topic model. COmapre and evaluate\n",
    "#assumed is post-constructed coherence. reuters is multitaged. so acq may have many alum also. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reuters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ba5c011ff576>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreuters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reuters' is not defined"
     ]
    }
   ],
   "source": [
    "reuters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test/14832', 'test/14858', 'test/14863', 'test/14882', 'test/15033', 'test/15043', 'test/15106', 'test/15287', 'test/15339', 'test/15341', 'test/15344', 'test/15351', 'test/15618', 'test/15648', 'test/15649', 'test/15676', 'test/15686', 'test/15720', 'test/15728', 'test/15845', 'test/15856', 'test/15860', 'test/15863', 'test/15871', 'test/15875', 'test/15877', 'test/15890', 'test/15904', 'test/15906', 'test/15910', 'test/15911', 'test/15917', 'test/15924', 'test/15952', 'test/15999', 'test/16012', 'test/16071', 'test/16099', 'test/16147', 'test/16194', 'test/16314', 'test/16525', 'test/16581', 'test/16624', 'test/16751', 'test/16765', 'test/17441', 'test/17446', 'test/17463', 'test/17472', 'test/17473', 'test/17477', 'test/17480', 'test/17486', 'test/17503', 'test/17509', 'test/17722', 'test/17767', 'test/17769', 'test/17783', 'test/17805', 'test/17878', 'test/18024', 'test/18035', 'test/18146', 'test/18228', 'test/18263', 'test/18311', 'test/18337', 'test/18367', 'test/18480', 'test/18482', 'test/18614', 'test/18858', 'test/18902', 'test/18908', 'test/18943', 'test/18945', 'test/18954', 'test/18973', 'test/19083', 'test/19165', 'test/19275', 'test/19431', 'test/19477', 'test/19497', 'test/19664', 'test/19668', 'test/19692', 'test/19721', 'test/19764', 'test/19821', 'test/20018', 'test/20045', 'test/20366', 'test/20406', 'test/20637', 'test/20645', 'test/20649', 'test/20723', 'test/20730', 'test/20763', 'test/21091', 'test/21243', 'test/21493', 'training/10038', 'training/10120', 'training/10139', 'training/10172', 'training/10175', 'training/10319', 'training/10339', 'training/10485', 'training/10487', 'training/10489', 'training/10519', 'training/1067', 'training/10701', 'training/10712', 'training/10720', 'training/1077', 'training/10860', 'training/10882', 'training/10956', 'training/11012', 'training/11070', 'training/11085', 'training/11091', 'training/11190', 'training/11208', 'training/11269', 'training/1131', 'training/11316', 'training/11392', 'training/11406', 'training/11436', 'training/11491', 'training/11607', 'training/11612', 'training/11699', 'training/11729', 'training/11739', 'training/11769', 'training/11836', 'training/11880', 'training/11885', 'training/11936', 'training/11939', 'training/11964', 'training/12002', 'training/12052', 'training/12055', 'training/1215', 'training/12160', 'training/12209', 'training/12311', 'training/12315', 'training/12323', 'training/12372', 'training/12417', 'training/12428', 'training/12436', 'training/12484', 'training/12500', 'training/12583', 'training/12587', 'training/1268', 'training/1273', 'training/12872', 'training/13099', 'training/13173', 'training/13179', 'training/1369', 'training/13744', 'training/13795', 'training/1385', 'training/13852', 'training/13856', 'training/13877', 'training/1395', 'training/1399', 'training/14483', 'training/14572', 'training/14698', 'training/14704', 'training/14732', 'training/1520', 'training/1582', 'training/1652', 'training/1660', 'training/1777', 'training/1843', 'training/1851', 'training/1856', 'training/1878', 'training/193', 'training/1952', 'training/197', 'training/1970', 'training/2044', 'training/2171', 'training/2172', 'training/2183', 'training/2186', 'training/2191', 'training/2217', 'training/2232', 'training/2264', 'training/235', 'training/2382', 'training/2436', 'training/2456', 'training/2595', 'training/2599', 'training/2617', 'training/2727', 'training/2741', 'training/2749', 'training/2777', 'training/2828', 'training/2848', 'training/2913', 'training/2922', 'training/2947', 'training/2957', 'training/3104', 'training/3132', 'training/3138', 'training/3163', 'training/3183', 'training/3191', 'training/3207', 'training/327', 'training/3282', 'training/3299', 'training/3306', 'training/3324', 'training/3330', 'training/3337', 'training/3358', 'training/3401', 'training/3429', 'training/3454', 'training/3847', 'training/3855', 'training/3862', 'training/3881', 'training/3949', 'training/395', 'training/3979', 'training/3981', 'training/3995', 'training/4047', 'training/4133', 'training/4280', 'training/4289', 'training/4296', 'training/4382', 'training/4490', 'training/4507', 'training/4599', 'training/4825', 'training/4905', 'training/4939', 'training/4988', 'training/5', 'training/5003', 'training/501', 'training/5017', 'training/5033', 'training/5037', 'training/5061', 'training/5109', 'training/5145', 'training/5153', 'training/516', 'training/5185', 'training/5338', 'training/5467', 'training/5518', 'training/5531', 'training/5606', 'training/5610', 'training/5630', 'training/5636', 'training/5637', 'training/5640', 'training/5683', 'training/57', 'training/5847', 'training/5933', 'training/6', 'training/6142', 'training/6221', 'training/6236', 'training/6239', 'training/6259', 'training/6269', 'training/6386', 'training/6451', 'training/6585', 'training/6588', 'training/6626', 'training/6735', 'training/6798', 'training/6836', 'training/6871', 'training/6890', 'training/6893', 'training/6897', 'training/694', 'training/6995', 'training/7062', 'training/7150', 'training/7152', 'training/7205', 'training/7215', 'training/7336', 'training/7387', 'training/7389', 'training/7390', 'training/7395', 'training/7397', 'training/7579', 'training/7671', 'training/7700', 'training/7775', 'training/7792', 'training/7842', 'training/7854', 'training/7917', 'training/7934', 'training/7943', 'training/7972', 'training/8004', 'training/8041', 'training/8112', 'training/8140', 'training/8161', 'training/8166', 'training/8213', 'training/8257', 'training/8273', 'training/829', 'training/8308', 'training/8400', 'training/8443', 'training/8446', 'training/8535', 'training/855', 'training/8759', 'training/8854', 'training/8877', 'training/8882', 'training/8941', 'training/8964', 'training/8983', 'training/8993', 'training/9039', 'training/9058', 'training/9093', 'training/9094', 'training/934', 'training/9470', 'training/9521', 'training/9614', 'training/9634', 'training/9667', 'training/97', 'training/9865', 'training/9912', 'training/9915', 'training/9958', 'training/9989']\n"
     ]
    }
   ],
   "source": [
    "# select some categories\n",
    "target = ['zinc', 'rye', 'coconut', 'corn', 'barley', 'gas', 'fuel'] #picked for many overlaps. and non-overlaps. \n",
    "fileids = reuters.fileids(target) #this gives us file-ids of the specific topics. \n",
    "print(fileids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374\n"
     ]
    }
   ],
   "source": [
    "print(len(fileids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THAI', 'TRADE', 'DEFICIT', 'WIDENS', 'IN', 'FIRST', ...]\n",
      "184\n"
     ]
    }
   ],
   "source": [
    "# get the words from the corpus \n",
    "#investigate the length of a document. get a sense of total possible vocab size\n",
    "corpus = [reuters.words(r) for r in fileids]\n",
    "print(corpus[0])\n",
    "print(len(corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232.860962567\n"
     ]
    }
   ],
   "source": [
    "# just an example of a way to investigate what the average length of documents\n",
    "# in a corpus is 232\n",
    "import numpy as np\n",
    "print(np.mean([len(corpus[i]) for i in range(len(corpus))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7270\n"
     ]
    }
   ],
   "source": [
    "all_words = set() \n",
    "for c in corpus: #goes through each document in the corpus. \n",
    "    all_words.update(set(c)) #what does update do? \n",
    "print(len(all_words))\n",
    "    #only has ~7000 words in the entire corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THAI TRADE DEFICIT WIDENS IN FIRST QUARTER Thailand ' s trade deficit widened to 4 . 5 billion baht in the first quarter of 1987 from 2 . 1 billion a year ago , the Business Economics Department said . It said Janunary / March imports rose to 65 . 1 billion baht from 58 . 7 billion . Thailand ' s improved business climate this year resulted in a 27 pct increase in imports of raw materials and semi - finished products . The country ' s oil import bill , however , fell 23 pct in the first quarter due to lower oil prices . The department said first quarter exports expanded to 60 . 6 billion baht from 56 . 6 billion . Export growth was smaller than expected due to lower earnings from many key commodities including rice whose earnings declined 18 pct , maize 66 pct , sugar 45 pct , tin 26 pct and canned pineapples seven pct . Products registering high export growth were jewellery up 64 pct , clothing 57 pct and rubber 35 pct .\n"
     ]
    }
   ],
   "source": [
    "# convert word arrays to strings \n",
    "docstrings = []\n",
    "for c in corpus:\n",
    "    _string = \" \".join([w for w in c]) #joins each array with a space. \n",
    "    docstrings.append(_string)\n",
    "print(docstrings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shan', \"you're\", \"wasn't\", 'from', 'any', 'needn', 'mustn', 'are', \"she's\", 'until', \"wouldn't\", 'y', 'for', 'how', 'your', \"that'll\", 'yourself', 'by', 'not', 'himself', 'during', 'into', 'ourselves', 'up', 'why', 'is', 'no', 'were', 'with', \"aren't\", 'most', \"weren't\", \"should've\", 'aren', \"mustn't\", 'this', 'just', 'nor', 'so', 'to', 'because', 'yourselves', 'haven', 'such', 'that', \"you've\", 'wouldn', 'been', 'will', 'wasn', 'above', 'it', 'after', \"won't\", 'now', 'theirs', 'off', 'shouldn', 'he', 'she', 'won', \"shouldn't\", 'whom', 've', 'over', 'too', 'these', 'what', \"doesn't\", 'again', 'do', 'each', \"it's\", 'don', \"you'll\", 'under', 'same', 'hers', 'mightn', 'him', 'of', 'on', 'should', 'hadn', \"shan't\", 'through', 'had', 're', 'me', 'between', 't', 'ain', \"didn't\", 'hasn', 'our', 'his', 'at', 'while', 'their', 's', 'they', 'we', 'couldn', \"needn't\", 'be', 'once', \"isn't\", 'you', 'does', 'isn', 'which', 'herself', 'being', 'her', 'yours', 'did', 'out', 'can', \"couldn't\", 'didn', 'other', 'as', 'there', 'against', 'or', 'very', 'am', 'm', 'themselves', 'than', 'here', 'weren', 'the', 'having', 'in', 'o', 'i', 'ma', 'when', 'its', 'where', 'an', 'only', \"haven't\", \"mightn't\", 'those', 'about', 'a', 'but', 'was', 'few', 'them', 'below', \"you'd\", 'more', 'all', 'who', 'if', 'both', \"hasn't\", 'has', 'then', 'doing', 'before', 'doesn', \"hadn't\", 'ours', 'have', 'own', 'itself', 'my', 'd', 'll', 'some', \"don't\", 'further', 'and', 'down', 'myself'}\n"
     ]
    }
   ],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.feature_extraction.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7b01591b015b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.feature_extraction.xml'"
     ]
    }
   ],
   "source": [
    "import sklearn.feature_extraction.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words = 'english', min_df=0.05) #word needs to be in 5% of documents to make the cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = cv.fit_transform(docstrings) #analyzes all of the docstrings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '10', '100', '11', '12', '13', '14', '15', '16', '17', '18', '19', '1985', '1986', '1987', '20', '200', '21', '22', '23', '24', '25', '26', '27', '28', '30', '300', '31', '33', '35', '36', '40', '400', '41', '43', '45', '48', '50', '500', '52', '55', '60', '600', '65', '700', '75', '80', '800', '85', '86', '87', '88', '90', '92', 'according', 'acreage', 'added', 'additional', 'administration', 'ago', 'agreement', 'agricultural', 'agriculture', 'analysts', 'announced', 'april', 'areas', 'asked', 'association', 'available', 'average', 'barley', 'barrels', 'based', 'began', 'billion', 'board', 'bushel', 'bushels', 'canada', 'canadian', 'cents', 'china', 'commission', 'committee', 'commodities', 'commodity', 'community', 'company', 'compared', 'contract', 'corn', 'corp', 'cost', 'countries', 'country', 'crop', 'crops', 'crude', 'current', 'currently', 'cut', 'day', 'december', 'delivery', 'demand', 'department', 'destinations', 'did', 'dlrs', 'domestic', 'earlier', 'ec', 'ecus', 'end', 'ended', 'estimated', 'estimates', 'european', 'expected', 'export', 'exporters', 'exports', 'fall', 'farm', 'farmers', 'february', 'feed', 'fell', 'figures', 'following', 'free', 'french', 'fuel', 'futures', 'gasoline', 'good', 'government', 'grain', 'grains', 'growers', 'harvest', 'heavy', 'high', 'higher', 'import', 'imports', 'including', 'increase', 'increased', 'industry', 'january', 'japan', 'july', 'june', 'late', 'lead', 'level', 'levels', 'likely', 'long', 'lower', 'lt', 'main', 'maize', 'major', 'make', 'march', 'market', 'marketing', 'ministry', 'mln', 'month', 'months', 'national', 'new', 'noted', 'official', 'officials', 'oil', 'output', 'pct', 'period', 'petroleum', 'previous', 'price', 'prices', 'private', 'probably', 'producers', 'production', 'products', 'program', 'purchase', 'purchases', 'recent', 'record', 'reduced', 'report', 'reported', 'reuters', 'rice', 'rise', 'rose', 'said', 'sales', 'says', 'season', 'secretary', 'seen', 'september', 'set', 'seven', 'shipments', 'similar', 'sold', 'sorghum', 'sources', 'south', 'soviet', 'soybean', 'soybeans', 'spokesman', 'state', 'states', 'stocks', 'supply', 'tender', 'time', 'today', 'told', 'tonne', 'tonnes', 'tons', 'total', 'trade', 'traders', 'union', 'united', 'usda', 'use', 'ussr', 'weather', 'week', 'weekly', 'wheat', 'world', 'year', 'years', 'yesterday', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(374, 251)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape) #term document matrix. shape tells you your x by y.  251 words in vocabulary. x is # documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(374, 738)\n"
     ]
    }
   ],
   "source": [
    "# too few features, change min_df to 0.02\n",
    "cv = CountVectorizer(stop_words = 'english', min_df = 0.02)\n",
    "X2 = cv.fit_transform(docstrings)\n",
    "# print(cv.get_feature_names())\n",
    "print(X2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# now let's model some topics. This is looking at the topics ehre. \n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "num_topics = len(target)\n",
    "lda = LatentDirichletAllocation(n_topics=num_topics, \n",
    "                                max_iter=5, \n",
    "                                learning_method='online', \n",
    "                                learning_offset=50.,\n",
    "                                random_state=0).fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "700 conservation 00 350 costs 125 caused 13 356 canada countries corn accepted contracts accord cause corporation 87 central commitments basis 59 85 agricultural chicago\n",
      "Topic 1:\n",
      "350 basis 125 35 120 considered community 000 analysts according 25 bushel conservation control 94 138 700 agricultural companies 16 06 200 17 150 10\n",
      "Topic 2:\n",
      "cause 700 bpd basis 90 budget countries contracts business agricultural 45 acres acreage 87 bushel 56 agreement british consumption 39 barrels analysts 25 76 costs\n",
      "Topic 3:\n",
      "bonus ago africa cause average 350 bread caused 68 base bpd community belt 72 125 american bushel agreement barrels case annual 53 budget analysts 138\n",
      "Topic 4:\n",
      "british 61 budget argentine 70 cause cover 85 90 belt agency 27 bonus 10 37 30 36 000 asked competitive 700 350 agricultural change 84\n",
      "Topic 5:\n",
      "basis bpd corporation cause community 53 countries 78 conservation considered aires ago 42 bushel commission 97 000 buys ccc barrel 76 central 06 77 bonus\n",
      "Topic 6:\n",
      "conservation 00 cause awarded basis 92 costs countries accepted 52 bpd coast close agricultural according cover affected 125 continued 250 bushel congress barrels april 700\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 25\n",
    "display_topics(lda, cv.get_feature_names(), no_top_words)\n",
    "\n",
    "#topic # does not correlate... with the category order we put. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "bushels 459 000 bales california admiralty carlos 550 793 authorised cases 1990 ban 09 calm 266 546 182 58 blast cane 215 alternatives 818 barley\n",
      "Topic 1:\n",
      "anticipated accord 182 155 carlos 143 258 266 avg bales 09 68 106 459 02 authorized 178 123 aires advantage asked 58 acid bushels 263\n",
      "Topic 2:\n",
      "182 affects alternatives 02 bales admiralty 09 58 cases 003 138 appropriations cause butyl 494 api 020 106 180 bp access 31 087 459 100\n",
      "Topic 3:\n",
      "admiralty bushels 000 bales activity alternatives 659 793 carlos cases cane bp 09 270 appropriations 33 butyl 266 825 490 61 75 488 boats 020\n",
      "Topic 4:\n",
      "bales 459 61 api blast admiralty anticipated activity 885 buyer bushels bourse 342 biggest 340 cajamarquilla 212 793 78 carlos buyers 879 addition cases 402\n",
      "Topic 5:\n",
      "37 api agpm benefit america access ahead 1982 190 194 780 achieved bales 1981 american 06 182 bureau 57 140 asia 412 anticipate 79 administrator\n",
      "Topic 6:\n",
      "bales alternatives approve cases 459 678 30 764 ahead 680 58 appropriations 215 breakdown api 78 32 addition alloyed 194 california approval 1989 affected brussels\n"
     ]
    }
   ],
   "source": [
    "# now try with the larger feature set\n",
    "lda = LatentDirichletAllocation(n_topics=num_topics, \n",
    "                                max_iter=5, \n",
    "                                learning_method='online', \n",
    "                                learning_offset=50.,\n",
    "                                random_state=0).fit(X2)\n",
    "\n",
    "no_top_words = 25\n",
    "display_topics(lda, cv.get_feature_names(), no_top_words) #this is getting closer. \n",
    "#need to identify \"commodity\" related words--the general overarching topic that's interering with topic modeling and remove those. Numbers are also appearing int he topics, this could be interfering with results, since frequency counts are the feature we are learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(374, 2867)\n"
     ]
    }
   ],
   "source": [
    "# let's try a different constraint - a term has to appear in at least two documents. this gives us a LOT more terms used. \n",
    "cv = CountVectorizer(stop_words = 'english', min_df=2)\n",
    "X3 = cv.fit_transform(docstrings)\n",
    "print(X3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "said corn pct mln nil dlrs year usda prices wheat department agriculture program 1986 grain 1987 sales week 87 oil report price acres production tonnes\n",
      "Topic 1:\n",
      "tonnes 000 said maize mln ec wheat barley export year zinc 1986 exports grain tonne french pct sources commission 500 ecus market 25 union january\n",
      "Topic 2:\n",
      "87 09 04 03 mln 1986 stocks 86 1985 10 exports imports total corn end use production start 50 supply price 40 12 23 nil\n",
      "Topic 3:\n",
      "87 09 police 04 tonnes 03 cuts kaunda said 86 formed 000 reductions people week price mln labour stocks exports zambia soybean imports production riots\n",
      "Topic 4:\n",
      "unit 87 components said gasoline 09 octane mobil tonnes dlrs 10 injuring upgrade upholds corn rules year mtbe 1986 kg fob refinery duty gulf 000\n",
      "Topic 5:\n",
      "mln pct said year crude barrels bpd stocks gasoline oil demand eia energy distillate rose ago 000 fuel week ended says api march petroleum smelter\n",
      "Topic 6:\n",
      "mln tonnes pct grain said soviet harvest crop rains production total week hectares area record crops maize 10 prices mm petrol estimates octane cents year\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=num_topics, max_iter=5, learning_method='online', \n",
    "                                learning_offset=50.,random_state=0).fit(X3)\n",
    "\n",
    "no_top_words = 25\n",
    "display_topics(lda, cv.get_feature_names(), no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(374, 5589)\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words = 'english')\n",
    "X4 = cv.fit_transform(docstrings)\n",
    "print(X4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "said corn pct mln nil dlrs year usda prices wheat department agriculture program 1986 grain 1987 sales week 87 oil report price acres production tonnes\n",
      "Topic 1:\n",
      "tonnes 000 said maize mln ec wheat barley export year zinc 1986 exports grain tonne french pct sources commission 500 ecus market 25 union january\n",
      "Topic 2:\n",
      "87 09 04 03 mln 1986 stocks 86 1985 10 exports imports total corn end use production start 50 supply price 40 12 23 nil\n",
      "Topic 3:\n",
      "87 09 police 04 tonnes 03 cuts kaunda said 86 formed 000 reductions people week price mln labour stocks exports zambia soybean imports production riots\n",
      "Topic 4:\n",
      "unit 87 components said gasoline 09 octane mobil tonnes dlrs 10 injuring upgrade upholds corn rules year mtbe 1986 kg fob refinery duty gulf 000\n",
      "Topic 5:\n",
      "mln pct said year crude barrels bpd stocks gasoline oil demand eia energy distillate rose ago 000 fuel week ended says api march petroleum smelter\n",
      "Topic 6:\n",
      "mln tonnes pct grain said soviet harvest crop rains production total week hectares area record crops maize 10 prices mm petrol estimates octane cents year\n"
     ]
    }
   ],
   "source": [
    " lda = LatentDirichletAllocation(n_topics=num_topics, max_iter=5, learning_method='online', \n",
    "                                learning_offset=50.,random_state=0).fit(X3)\n",
    "\n",
    "no_top_words = 25\n",
    "display_topics(lda, cv.get_feature_names(), no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_top_words = 100 #hypothesis is that there is not enough differentiation between topics. see whether coherence is poor. \n",
    "features = cv.get_feature_names()  #basically IDs \n",
    "top_100_by_topic = dict()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    terms = set([features[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "    top_100_by_topic[topic_idx] = terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '003',\n",
       " '007',\n",
       " '008',\n",
       " '01',\n",
       " '02',\n",
       " '020',\n",
       " '025',\n",
       " '028',\n",
       " '03',\n",
       " '030',\n",
       " '04',\n",
       " '040',\n",
       " '044',\n",
       " '05',\n",
       " '050',\n",
       " '059',\n",
       " '06',\n",
       " '062',\n",
       " '067',\n",
       " '07',\n",
       " '070',\n",
       " '075',\n",
       " '08',\n",
       " '087',\n",
       " '09',\n",
       " '090',\n",
       " '098',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '101',\n",
       " '102',\n",
       " '103',\n",
       " '104',\n",
       " '105',\n",
       " '106',\n",
       " '107',\n",
       " '108',\n",
       " '109',\n",
       " '11',\n",
       " '110',\n",
       " '111',\n",
       " '113',\n",
       " '115',\n",
       " '117',\n",
       " '118',\n",
       " '119',\n",
       " '12',\n",
       " '120',\n",
       " '122',\n",
       " '123',\n",
       " '124',\n",
       " '125',\n",
       " '126',\n",
       " '127',\n",
       " '128',\n",
       " '129',\n",
       " '13',\n",
       " '130',\n",
       " '131',\n",
       " '132',\n",
       " '133',\n",
       " '134',\n",
       " '135',\n",
       " '136',\n",
       " '137',\n",
       " '138',\n",
       " '139',\n",
       " '14',\n",
       " '140',\n",
       " '141',\n",
       " '142',\n",
       " '143',\n",
       " '145',\n",
       " '147',\n",
       " '148',\n",
       " '149',\n",
       " '15',\n",
       " '150',\n",
       " '151',\n",
       " '152',\n",
       " '153',\n",
       " '154',\n",
       " '155',\n",
       " '156',\n",
       " '157',\n",
       " '159',\n",
       " '16',\n",
       " '160',\n",
       " '161',\n",
       " '163',\n",
       " '164',\n",
       " '165',\n",
       " '166',\n",
       " '167',\n",
       " '168',\n",
       " '17',\n",
       " '170',\n",
       " '171',\n",
       " '175',\n",
       " '178',\n",
       " '179',\n",
       " '18',\n",
       " '180',\n",
       " '182',\n",
       " '187',\n",
       " '19',\n",
       " '190',\n",
       " '191',\n",
       " '192',\n",
       " '194',\n",
       " '1978',\n",
       " '1979',\n",
       " '1980',\n",
       " '1981',\n",
       " '1982',\n",
       " '1983',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1989',\n",
       " '1990',\n",
       " '1992',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '201',\n",
       " '203',\n",
       " '204',\n",
       " '206',\n",
       " '21',\n",
       " '210',\n",
       " '211',\n",
       " '212',\n",
       " '213',\n",
       " '215',\n",
       " '217',\n",
       " '218',\n",
       " '22',\n",
       " '220',\n",
       " '222',\n",
       " '225',\n",
       " '229',\n",
       " '23',\n",
       " '233',\n",
       " '24',\n",
       " '240',\n",
       " '241',\n",
       " '244',\n",
       " '245',\n",
       " '247',\n",
       " '248',\n",
       " '25',\n",
       " '250',\n",
       " '251',\n",
       " '253',\n",
       " '254',\n",
       " '257',\n",
       " '258',\n",
       " '26',\n",
       " '260',\n",
       " '263',\n",
       " '266',\n",
       " '27',\n",
       " '270',\n",
       " '274',\n",
       " '275',\n",
       " '28',\n",
       " '287',\n",
       " '289',\n",
       " '29',\n",
       " '295',\n",
       " '296',\n",
       " '30',\n",
       " '300',\n",
       " '301',\n",
       " '304',\n",
       " '31',\n",
       " '310',\n",
       " '313',\n",
       " '315',\n",
       " '317',\n",
       " '32',\n",
       " '320',\n",
       " '321',\n",
       " '323',\n",
       " '325',\n",
       " '33',\n",
       " '332',\n",
       " '333',\n",
       " '334',\n",
       " '335',\n",
       " '336',\n",
       " '34',\n",
       " '340',\n",
       " '342',\n",
       " '347',\n",
       " '35',\n",
       " '350',\n",
       " '355',\n",
       " '356',\n",
       " '36',\n",
       " '360',\n",
       " '362',\n",
       " '37',\n",
       " '375',\n",
       " '38',\n",
       " '385',\n",
       " '387',\n",
       " '39',\n",
       " '390',\n",
       " '391',\n",
       " '392',\n",
       " '398',\n",
       " '40',\n",
       " '400',\n",
       " '402',\n",
       " '403',\n",
       " '405',\n",
       " '408',\n",
       " '41',\n",
       " '410',\n",
       " '412',\n",
       " '413',\n",
       " '417',\n",
       " '419',\n",
       " '42',\n",
       " '420',\n",
       " '425',\n",
       " '426',\n",
       " '429',\n",
       " '43',\n",
       " '433',\n",
       " '44',\n",
       " '45',\n",
       " '450',\n",
       " '457',\n",
       " '459',\n",
       " '46',\n",
       " '460',\n",
       " '47',\n",
       " '472',\n",
       " '475',\n",
       " '477',\n",
       " '48',\n",
       " '480',\n",
       " '481',\n",
       " '487',\n",
       " '488',\n",
       " '49',\n",
       " '490',\n",
       " '494',\n",
       " '498',\n",
       " '50',\n",
       " '500',\n",
       " '51',\n",
       " '510',\n",
       " '516',\n",
       " '517',\n",
       " '518',\n",
       " '52',\n",
       " '520',\n",
       " '521',\n",
       " '524',\n",
       " '528',\n",
       " '529',\n",
       " '53',\n",
       " '54',\n",
       " '543',\n",
       " '546',\n",
       " '55',\n",
       " '550',\n",
       " '554',\n",
       " '56',\n",
       " '560',\n",
       " '561',\n",
       " '563',\n",
       " '564',\n",
       " '567',\n",
       " '57',\n",
       " '572',\n",
       " '575',\n",
       " '58',\n",
       " '580',\n",
       " '587',\n",
       " '59',\n",
       " '591',\n",
       " '598',\n",
       " '60',\n",
       " '600',\n",
       " '603',\n",
       " '607',\n",
       " '61',\n",
       " '610',\n",
       " '615',\n",
       " '616',\n",
       " '62',\n",
       " '620',\n",
       " '623',\n",
       " '625',\n",
       " '628',\n",
       " '63',\n",
       " '630',\n",
       " '635',\n",
       " '637',\n",
       " '64',\n",
       " '641',\n",
       " '648',\n",
       " '65',\n",
       " '650',\n",
       " '658',\n",
       " '659',\n",
       " '66',\n",
       " '662',\n",
       " '663',\n",
       " '67',\n",
       " '675',\n",
       " '678',\n",
       " '68',\n",
       " '680',\n",
       " '682',\n",
       " '683',\n",
       " '688',\n",
       " '69',\n",
       " '692',\n",
       " '693',\n",
       " '70',\n",
       " '700',\n",
       " '703',\n",
       " '71',\n",
       " '718',\n",
       " '72',\n",
       " '720',\n",
       " '725',\n",
       " '727',\n",
       " '729',\n",
       " '73',\n",
       " '732',\n",
       " '734',\n",
       " '74',\n",
       " '75',\n",
       " '750',\n",
       " '76',\n",
       " '760',\n",
       " '764',\n",
       " '767',\n",
       " '77',\n",
       " '770',\n",
       " '772',\n",
       " '774',\n",
       " '78',\n",
       " '780',\n",
       " '79',\n",
       " '793',\n",
       " '80',\n",
       " '800',\n",
       " '805',\n",
       " '808',\n",
       " '81',\n",
       " '818',\n",
       " '82',\n",
       " '825',\n",
       " '83',\n",
       " '830',\n",
       " '837',\n",
       " '84',\n",
       " '842',\n",
       " '847',\n",
       " '848',\n",
       " '85',\n",
       " '850',\n",
       " '86',\n",
       " '869',\n",
       " '87',\n",
       " '870',\n",
       " '875',\n",
       " '879',\n",
       " '88',\n",
       " '880',\n",
       " '885',\n",
       " '887',\n",
       " '89',\n",
       " '892',\n",
       " '893',\n",
       " '894',\n",
       " '896',\n",
       " '90',\n",
       " '900',\n",
       " '91',\n",
       " '910',\n",
       " '919',\n",
       " '92',\n",
       " '920',\n",
       " '93',\n",
       " '94',\n",
       " '944',\n",
       " '95',\n",
       " '950',\n",
       " '959',\n",
       " '96',\n",
       " '960',\n",
       " '967',\n",
       " '97',\n",
       " '974',\n",
       " '98',\n",
       " '980',\n",
       " '984',\n",
       " '99',\n",
       " '992',\n",
       " 'abastecimento',\n",
       " 'able',\n",
       " 'abolish',\n",
       " 'accept',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accession',\n",
       " 'accord',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accounted',\n",
       " 'accounting',\n",
       " 'accounts',\n",
       " 'achieved',\n",
       " 'acid',\n",
       " 'acknowledged',\n",
       " 'acquisition',\n",
       " 'acre',\n",
       " 'acreage',\n",
       " 'acres',\n",
       " 'act',\n",
       " 'action',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activity',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'added',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additions',\n",
       " 'adds',\n",
       " 'adequate',\n",
       " 'adjust',\n",
       " 'adjusted',\n",
       " 'adjustment',\n",
       " 'adjustments',\n",
       " 'adm',\n",
       " 'administration',\n",
       " 'administrator',\n",
       " 'admiralty',\n",
       " 'adopted',\n",
       " 'adopts',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advantage',\n",
       " 'advantageous',\n",
       " 'adverse',\n",
       " 'afbf',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affects',\n",
       " 'afford',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'aggressive',\n",
       " 'ago',\n",
       " 'agpm',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'agreement',\n",
       " 'agrees',\n",
       " 'agrianalysis',\n",
       " 'agricultural',\n",
       " 'agriculture',\n",
       " 'ahead',\n",
       " 'aid',\n",
       " 'aide',\n",
       " 'aides',\n",
       " 'aim',\n",
       " 'aimed',\n",
       " 'air',\n",
       " 'aires',\n",
       " 'alaska',\n",
       " 'albert',\n",
       " 'algeria',\n",
       " 'allocated',\n",
       " 'allow',\n",
       " 'allowance',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'alloyed',\n",
       " 'alternatives',\n",
       " 'aluminum',\n",
       " 'amendment',\n",
       " 'america',\n",
       " 'american',\n",
       " 'amoco',\n",
       " 'amounted',\n",
       " 'amounts',\n",
       " 'ample',\n",
       " 'amselco',\n",
       " 'amstutz',\n",
       " 'analysis',\n",
       " 'analyst',\n",
       " 'analysts',\n",
       " 'animal',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'annual',\n",
       " 'annually',\n",
       " 'answer',\n",
       " 'anticipate',\n",
       " 'anticipated',\n",
       " 'anticipation',\n",
       " 'api',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appeared',\n",
       " 'appears',\n",
       " 'application',\n",
       " 'applied',\n",
       " 'apply',\n",
       " 'approach',\n",
       " 'approaching',\n",
       " 'appropriate',\n",
       " 'appropriations',\n",
       " 'approval',\n",
       " 'approve',\n",
       " 'approved',\n",
       " 'approves',\n",
       " 'approximately',\n",
       " 'apr',\n",
       " 'april',\n",
       " 'arabia',\n",
       " 'archer',\n",
       " 'archie',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'argentina',\n",
       " 'argentine',\n",
       " 'argued',\n",
       " 'arkansas',\n",
       " 'arm',\n",
       " 'arrive',\n",
       " 'asa',\n",
       " 'ascs',\n",
       " 'asia',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'assaying',\n",
       " 'assess',\n",
       " 'assigned',\n",
       " 'assistant',\n",
       " 'associated',\n",
       " 'association',\n",
       " 'assuming',\n",
       " 'attacking',\n",
       " 'attempt',\n",
       " 'attempting',\n",
       " 'attending',\n",
       " 'attention',\n",
       " 'aug',\n",
       " 'august',\n",
       " 'austerity',\n",
       " 'australia',\n",
       " 'authorise',\n",
       " 'authorised',\n",
       " 'authorises',\n",
       " 'authorities',\n",
       " 'authority',\n",
       " 'authorizations',\n",
       " 'authorized',\n",
       " 'availability',\n",
       " 'available',\n",
       " 'average',\n",
       " 'averaged',\n",
       " 'avert',\n",
       " 'avg',\n",
       " 'avge',\n",
       " 'avoid',\n",
       " 'awarded',\n",
       " 'awards',\n",
       " 'away',\n",
       " 'bad',\n",
       " 'badly',\n",
       " 'balance',\n",
       " 'bales',\n",
       " 'ball',\n",
       " 'ban',\n",
       " 'bangladesh',\n",
       " 'bank',\n",
       " 'bargaining',\n",
       " 'barge',\n",
       " 'barges',\n",
       " 'barley',\n",
       " 'barrel',\n",
       " 'barrels',\n",
       " 'barring',\n",
       " 'base',\n",
       " 'based',\n",
       " 'basic',\n",
       " 'basin',\n",
       " 'basis',\n",
       " 'bay',\n",
       " 'bbls',\n",
       " 'beans',\n",
       " 'bearish',\n",
       " 'beaumont',\n",
       " 'beef',\n",
       " 'beet',\n",
       " 'began',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'begins',\n",
       " 'belgium',\n",
       " 'belgrade',\n",
       " 'belief',\n",
       " 'believe',\n",
       " 'believed',\n",
       " 'believes',\n",
       " 'belt',\n",
       " 'benefit',\n",
       " 'benefits',\n",
       " 'best',\n",
       " 'better',\n",
       " 'beutel',\n",
       " 'beverage',\n",
       " 'bid',\n",
       " 'bids',\n",
       " 'big',\n",
       " 'biggest',\n",
       " 'billion',\n",
       " 'bismuth',\n",
       " 'blast',\n",
       " 'board',\n",
       " 'boats',\n",
       " 'bob',\n",
       " 'body',\n",
       " 'bond',\n",
       " 'bonus',\n",
       " 'book',\n",
       " 'booked',\n",
       " 'boost',\n",
       " 'boosting',\n",
       " 'boosts',\n",
       " 'bought',\n",
       " 'bourse',\n",
       " 'bp',\n",
       " 'bpd',\n",
       " 'brackets',\n",
       " 'brand',\n",
       " 'brasilia',\n",
       " 'brazil',\n",
       " 'brazilian',\n",
       " 'bread',\n",
       " 'breakdown',\n",
       " 'breakdowns',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'brings',\n",
       " 'britain',\n",
       " 'british',\n",
       " 'broad',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brokers',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'brussels',\n",
       " 'bu',\n",
       " 'budget',\n",
       " 'buenos',\n",
       " 'build',\n",
       " 'bulk',\n",
       " 'bulletin',\n",
       " 'bullish',\n",
       " 'bumper',\n",
       " 'bureau',\n",
       " 'burnham',\n",
       " 'bushel',\n",
       " 'bushels',\n",
       " 'business',\n",
       " 'butyl',\n",
       " 'buy',\n",
       " 'buyer',\n",
       " 'buyers',\n",
       " 'buying',\n",
       " 'buys',\n",
       " 'cabinet',\n",
       " 'cadmium',\n",
       " 'cajamarquilla',\n",
       " 'cake',\n",
       " 'calculated',\n",
       " 'calculation',\n",
       " 'calendar',\n",
       " 'california',\n",
       " 'called',\n",
       " 'calls',\n",
       " 'calm',\n",
       " 'caltex',\n",
       " 'came',\n",
       " 'campaign',\n",
       " 'canada',\n",
       " 'canadian',\n",
       " 'cane',\n",
       " 'capacity',\n",
       " 'capital',\n",
       " 'cargill',\n",
       " 'cargo',\n",
       " 'cargoes',\n",
       " 'carlos',\n",
       " 'carried',\n",
       " 'carries',\n",
       " 'carryover',\n",
       " 'cartel',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'cash',\n",
       " 'catalysts',\n",
       " 'catalytic',\n",
       " 'cattle',\n",
       " 'cattlemen',\n",
       " 'cause',\n",
       " 'caused',\n",
       " 'causing',\n",
       " 'cbt',\n",
       " 'ccc',\n",
       " 'cent',\n",
       " 'centigrade',\n",
       " 'central',\n",
       " 'centre',\n",
       " 'cents',\n",
       " 'cereais',\n",
       " 'cereal',\n",
       " 'cereals',\n",
       " 'certain',\n",
       " 'certificate',\n",
       " 'certificates',\n",
       " 'certs',\n",
       " 'cftc',\n",
       " 'chaco',\n",
       " 'chairman',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changes',\n",
       " 'changing',\n",
       " 'charge',\n",
       " 'charles',\n",
       " 'cheap',\n",
       " 'cheaper',\n",
       " 'check',\n",
       " 'chemical',\n",
       " 'chicago',\n",
       " 'chief',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'choose',\n",
       " 'cigra',\n",
       " 'cincinnati',\n",
       " 'circulated',\n",
       " 'cited',\n",
       " 'citing',\n",
       " 'city',\n",
       " 'civil',\n",
       " 'claim',\n",
       " 'claims',\n",
       " 'classifications',\n",
       " 'clause',\n",
       " 'clauses',\n",
       " 'clayton',\n",
       " 'clear',\n",
       " 'cleopa',\n",
       " 'climate',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closely',\n",
       " 'clt',\n",
       " 'cnp',\n",
       " 'coal',\n",
       " 'coarse',\n",
       " 'coast',\n",
       " 'coastal',\n",
       " 'coconut',\n",
       " 'code',\n",
       " 'coffee',\n",
       " 'cold',\n",
       " 'collapse',\n",
       " 'colombia',\n",
       " 'columbia',\n",
       " 'combined',\n",
       " 'come',\n",
       " 'comercial',\n",
       " 'comes',\n",
       " 'cominco',\n",
       " 'coming',\n",
       " 'comment',\n",
       " 'comments',\n",
       " 'commercial',\n",
       " 'commission',\n",
       " 'commitment',\n",
       " 'commitments',\n",
       " 'committed',\n",
       " 'committee',\n",
       " 'committees',\n",
       " 'commodities',\n",
       " 'commodity',\n",
       " 'community',\n",
       " 'companies',\n",
       " 'company',\n",
       " 'compared',\n",
       " 'compares',\n",
       " 'comparisons',\n",
       " 'compensate',\n",
       " 'compensation',\n",
       " 'compete',\n",
       " 'competing',\n",
       " 'competition',\n",
       " 'competitive',\n",
       " 'competitively',\n",
       " 'competitiveness',\n",
       " 'competitor',\n",
       " 'complaint',\n",
       " 'complete',\n",
       " 'completed',\n",
       " 'completion',\n",
       " 'complex',\n",
       " 'components',\n",
       " 'compound',\n",
       " 'compounding',\n",
       " 'comprised',\n",
       " 'comprises',\n",
       " 'comprising',\n",
       " 'concentrate',\n",
       " 'concentrated',\n",
       " 'concern',\n",
       " 'concerns',\n",
       " 'concluded',\n",
       " 'condition',\n",
       " 'conditions',\n",
       " 'conference',\n",
       " 'confirmation',\n",
       " 'confirmed',\n",
       " 'confusion',\n",
       " 'congress',\n",
       " 'congressional',\n",
       " 'connecticut',\n",
       " 'conoco',\n",
       " 'consequences',\n",
       " 'consequently',\n",
       " 'conservation',\n",
       " 'consider',\n",
       " 'considerably',\n",
       " 'consideration',\n",
       " 'considered',\n",
       " 'considering',\n",
       " 'consisted',\n",
       " 'construction',\n",
       " 'consumed',\n",
       " 'consumer',\n",
       " 'consumers',\n",
       " 'consumption',\n",
       " 'contained',\n",
       " 'containing',\n",
       " 'contended',\n",
       " 'content',\n",
       " 'context',\n",
       " 'continental',\n",
       " 'continue',\n",
       " 'continued',\n",
       " 'continues',\n",
       " 'continuing',\n",
       " 'continuous',\n",
       " 'contract',\n",
       " 'contracts',\n",
       " 'contrast',\n",
       " 'control',\n",
       " 'controlled',\n",
       " 'controls',\n",
       " 'controversial',\n",
       " 'converts',\n",
       " 'convince',\n",
       " 'convinced',\n",
       " 'cool',\n",
       " 'cooperatives',\n",
       " 'copper',\n",
       " 'copra',\n",
       " 'cordoba',\n",
       " 'corn',\n",
       " 'corp',\n",
       " 'corporation',\n",
       " 'correct',\n",
       " 'corrected',\n",
       " 'corresponding',\n",
       " 'corrientes',\n",
       " 'cost',\n",
       " 'costs',\n",
       " 'cotton',\n",
       " 'cottonseed',\n",
       " 'council',\n",
       " 'counselor',\n",
       " 'countervailing',\n",
       " 'counties',\n",
       " 'countries',\n",
       " 'country',\n",
       " 'county',\n",
       " 'coupled',\n",
       " 'course',\n",
       " 'court',\n",
       " 'cover',\n",
       " 'coverage',\n",
       " 'covered',\n",
       " 'covering',\n",
       " 'covers',\n",
       " 'create',\n",
       " 'creating',\n",
       " 'credit',\n",
       " 'credits',\n",
       " 'creek',\n",
       " 'crisis',\n",
       " 'criticism',\n",
       " 'crop',\n",
       " 'cropland',\n",
       " 'crops',\n",
       " 'crown',\n",
       " 'crucial',\n",
       " 'crude',\n",
       " 'crudes',\n",
       " 'crushings',\n",
       " 'cst',\n",
       " 'cts',\n",
       " 'cubic',\n",
       " 'cumulative',\n",
       " 'currency',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'customer',\n",
       " 'customers',\n",
       " 'customs',\n",
       " 'cut',\n",
       " 'cuts',\n",
       " 'cutting',\n",
       " 'cwt',\n",
       " 'cwts',\n",
       " 'cycle',\n",
       " 'cyprus',\n",
       " 'daily',\n",
       " 'dairy',\n",
       " 'dale',\n",
       " 'damage',\n",
       " 'damaged',\n",
       " 'damages',\n",
       " 'dampen',\n",
       " 'dan',\n",
       " 'danger',\n",
       " 'dangerous',\n",
       " 'daniel',\n",
       " 'daniels',\n",
       " 'data',\n",
       " 'date',\n",
       " 'dated',\n",
       " 'davenport',\n",
       " 'david',\n",
       " 'davy',\n",
       " 'day',\n",
       " 'days',\n",
       " 'dd',\n",
       " 'deadline',\n",
       " 'deal',\n",
       " 'dealer',\n",
       " 'dealers',\n",
       " 'dealing',\n",
       " 'debate',\n",
       " 'debt',\n",
       " 'dec',\n",
       " 'decades',\n",
       " 'december',\n",
       " 'decide',\n",
       " 'decided',\n",
       " 'decision',\n",
       " 'decisions',\n",
       " 'declare',\n",
       " 'decline',\n",
       " 'declined',\n",
       " 'declines',\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "27\n",
      "26\n",
      "32\n",
      "24\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "s = top_100_by_topic[0]\n",
    "for i in range(1,7):\n",
    "    t = top_100_by_topic[i]\n",
    "    ins = s.intersection(t) #measure intersection of number 0 to the other 6. \n",
    "    print(len(ins))\n",
    "    #this is 5 diff numbers. thse are percentages of simiarlites. This means 25-33\n",
    "    #5 of words appearing int op 100 of every topic, so coherence is poor. good idea to add these to stop words list. \n",
    "    # if in same position for 2 topics, then huge problem. this can help you find the topics that are similar. this means there is ahuge overlap. \n",
    "    \n",
    "    #print the intersection in this loop. \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10',\n",
       " '15',\n",
       " '1986',\n",
       " '20',\n",
       " '30',\n",
       " '87',\n",
       " 'corn',\n",
       " 'crop',\n",
       " 'expected',\n",
       " 'gasoline',\n",
       " 'grain',\n",
       " 'higher',\n",
       " 'mln',\n",
       " 'month',\n",
       " 'new',\n",
       " 'official',\n",
       " 'oil',\n",
       " 'pct',\n",
       " 'price',\n",
       " 'prices',\n",
       " 'production',\n",
       " 'said',\n",
       " 'season',\n",
       " 'soybean',\n",
       " 'tonnes',\n",
       " 'total',\n",
       " 'trade',\n",
       " 'ussr',\n",
       " 'week',\n",
       " 'wheat',\n",
       " 'year'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.intersection(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "25\n",
      "28\n",
      "27\n",
      "23\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "s = top_100_by_topic[1]\n",
    "for i in [i for i in range(0,7) if i != 1]:\n",
    "    t = top_100_by_topic[i]\n",
    "    ins = s.intersection(t)\n",
    "    print(len(ins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'25', '75', 'company', 'march', 'exports', 'agriculture', 'total', 'mln', 'pct', 'tonnes', '1987', 'grain', 'wheat', '87', 'export', 'traders', 'production', 'said', 'department', '15', 'import', 'market', '10', 'trade', 'dlrs', 'corn', 'season', 'imports', '1986', '000', 'year', '20', '30'}\n",
      "{'25', '75', 'exports', 'usda', 'total', 'mln', 'tonnes', 'nil', 'grain', 'wheat', '87', 'price', 'production', '50', 'average', '15', 'acres', 'marketing', '10', 'dlrs', 'corn', 'season', 'imports', '1986', '20', '30', 'soybean'}\n",
      "{'25', 'exports', 'total', 'mln', 'tonnes', 'wheat', '87', 'price', 'production', 'said', 'average', 'marketing', '10', 'corn', 'season', 'imports', '1986', '000', 'sales', 'government', 'year', 'week', '20', '30', 'soybean', 'reported'}\n",
      "{'canada', 'high', 'farmers', '75', 'company', 'canadian', 'total', 'mln', 'tonnes', 'nil', 'grain', 'wheat', 'gulf', '87', 'lt', 'price', 'production', '50', 'said', '15', '10', 'gasoline', 'dlrs', 'corn', 'imports', '1986', '000', 'year', 'week', 'crop', '20', '30'}\n",
      "{'oil', 'fuel', 'march', 'pct', 'mln', 'production', 'billion', 'said', 'department', '15', 'report', 'gasoline', 'analysts', 'imports', '1986', '000', 'prices', 'higher', 'year', 'increase', 'week', 'products', '20', 'earlier'}\n",
      "{'oil', 'official', 'total', 'mln', 'pct', 'tonnes', 'grain', 'wheat', 'expected', '87', 'price', 'production', 'said', '15', '10', 'gasoline', 'trade', 'corn', 'season', '1986', 'month', 'prices', 'higher', 'new', 'year', 'week', 'ussr', 'crop', '20', '30', 'soybean'}\n"
     ]
    }
   ],
   "source": [
    "s = top_100_by_topic[0]\n",
    "for i in range(1,7):\n",
    "    t = top_100_by_topic[i]\n",
    "    ins = s.intersection(t)\n",
    "    print(ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
